#!/usr/bin/env python3

from typing import Optional, List, Tuple, Any
from picamera2 import Picamera2
from datetime import datetime
import time
import os
from pathlib import Path
import subprocess
import shutil
import google.generativeai as genai
from dotenv import load_dotenv
from PIL import Image
import soundfile as sf
import warnings
import sounddevice as sd
import librosa
import numpy as np
import re
from urllib.parse import urlencode
from urllib.request import urlopen
from urllib.error import URLError

RECORDING_SAMPLE_RATE = 48000
TARGET_SAMPLE_RATE = 16000


os.environ["ORT_DISABLE_ALL_PROVIDERS"] = "0"
warnings.filterwarnings("ignore", category=UserWarning, module="onnxruntime")

# USBãƒã‚¤ã‚¯ã®ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆè¨­å®š
sd.default.samplerate = RECORDING_SAMPLE_RATE
sd.default.channels = 1

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# Gemini APIã®è¨­å®š
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel("gemini-2.0-flash")

# ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰
VOICE_EXAMPLES_DIR = "./voice_examples_16k"
WAKE_THRESHOLD = 0.04  # æ¤œå‡ºé–¾å€¤ï¼ˆå°ã•ã„ã»ã©å³æ ¼ã€å¤§ãã„ã»ã©ç·©ã„ï¼‰
RECORDING_DURATION = 5  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
REFERENCE_AUDIO_LENGTH = 2.5  # ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹éŸ³å£°ã®çµ±ä¸€é•·ï¼ˆç§’ï¼‰
RAG_PROMPT_FILE = "./rag_prompt.txt"  # RAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
PHOTO_DIR = "./Past_Photo"  # å†™çœŸä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PROMPT_DIR = "./Past_Prompt"  # éŸ³å£°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
AQUESTALK_PATH = "./aquestalkpi/AquesTalkPi"
AQUESTALK_DEVICE = "plughw:1,0"
SERVO_API_BASE_URL = "http://172.20.10.3"
SERVO_AXIS_CHANNEL_MAP = {"x": 0, "y": 1}
SERVO_MIN_ANGLE = 0
SERVO_MAX_ANGLE = 180
SERVO_COMMAND_PATTERN = re.compile(r'req\.servo\s*\(([^)]*)\)', re.IGNORECASE)
SERVO_AXIS_PATTERN = re.compile(r'axis\s*=\s*[\'"]?\s*([xy])', re.IGNORECASE)
SERVO_ANGLE_PATTERN = re.compile(r'angle\s*(?:=|:)\s*([-+]?\s*\d+)', re.IGNORECASE)

# éŸ³å£°æ¤œå‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
VAD_SILENCE_THRESHOLD = 0.01  # ç„¡éŸ³åˆ¤å®šã®é–¾å€¤ï¼ˆRMSï¼‰
VAD_SILENCE_DURATION = 1.5  # ã“ã®ç§’æ•°ç„¡éŸ³ãŒç¶šã„ãŸã‚‰åœæ­¢ï¼ˆç§’ï¼‰
VAD_MIN_DURATION = 0.5  # æœ€ä½éŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
VAD_CHUNK_SIZE = 4800  # éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã€ç´„0.1ç§’åˆ†ï¼‰


def take_photo(filename: Optional[str] = None) -> str:
    try:
        os.makedirs(PHOTO_DIR, exist_ok=True)
        picam2 = Picamera2()
        camera_config = picam2.create_still_configuration()
        picam2.configure(camera_config)
        picam2.start()
        time.sleep(2)
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"photo_{timestamp}.jpg"
        
        filepath = os.path.join(PHOTO_DIR, filename)
        picam2.capture_file(filepath)
        print(f"å†™çœŸã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
        
        picam2.stop()
        picam2.close()
        
        return filepath
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: å†™çœŸã®æ’®å½±ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise RuntimeError(f"å†™çœŸã®æ’®å½±ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}") from e


def load_rag_prompt() -> str:
    if os.path.exists(RAG_PROMPT_FILE):
        with open(RAG_PROMPT_FILE, 'r', encoding='utf-8') as f:
            rag_content = f.read().strip()
            if rag_content:
                print(f"RAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(rag_content)}æ–‡å­—ï¼‰")
                return rag_content
    return ""


def analyze_photo_with_gemini(image_path: str, prompt: str = "", use_rag: bool = True) -> str:
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
    
    try:
        image = Image.open(image_path)
        
        rag_prompt = ""
        if use_rag:
            rag_prompt = load_rag_prompt()
        
        if rag_prompt:
            if rag_prompt.strip().endswith("/////"):
                full_prompt = f"""{rag_prompt}
{prompt}""" if prompt else rag_prompt
            else:
                full_prompt = f"""{rag_prompt}

/////
{prompt}""" if prompt else f"""{rag_prompt}

/////"""
        else:
            full_prompt = prompt if prompt else "ã“ã®ç”»åƒã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
        
        print(f"\nGemini AIã§ç”»åƒã‚’åˆ†æä¸­...")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        if rag_prompt:
            print(f"ï¼ˆRAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {len(rag_prompt)}æ–‡å­—ï¼‰")
        
        response = model.generate_content([full_prompt, image])
        
        return response.text
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: Gemini APIã®å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise RuntimeError(f"Gemini APIã®å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}") from e


def synthesize_speech(text: str) -> None:
    if not text:
        return

    try:
        clean_text = text.replace("\n", " ").strip()
        if not clean_text:
            return

        if not os.path.exists(AQUESTALK_PATH):
            print(f"ã‚¨ãƒ©ãƒ¼: éŸ³å£°åˆæˆãƒã‚¤ãƒŠãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {AQUESTALK_PATH}")
            return

        if shutil.which("aplay") is None:
            print("ã‚¨ãƒ©ãƒ¼: aplay ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        tts_result = subprocess.run(
            [AQUESTALK_PATH, clean_text],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        if tts_result.returncode != 0:
            error_msg = tts_result.stderr.decode(errors="ignore") if tts_result.stderr else "Unknown error"
            print(f"ã‚¨ãƒ©ãƒ¼: AquesTalkPi ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg.strip()}")
            return

        if not tts_result.stdout:
            print("ã‚¨ãƒ©ãƒ¼: AquesTalkPi ã‹ã‚‰éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return

        aplay_result = subprocess.run(
            ["aplay", "-D", AQUESTALK_DEVICE],
            input=tts_result.stdout,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE,
        )

        if aplay_result.returncode != 0:
            error_msg = aplay_result.stderr.decode(errors="ignore") if aplay_result.stderr else "Unknown error"
            print(f"ã‚¨ãƒ©ãƒ¼: aplay ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg.strip()}")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: éŸ³å£°åˆæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def send_servo_command(axis: str, angle: int) -> None:
    channel = SERVO_AXIS_CHANNEL_MAP.get(axis)
    if channel is None:
        print(f"è­¦å‘Š: æœªå¯¾å¿œã®è»¸ {axis} ã‚’å—ä¿¡ã—ã¾ã—ãŸ")
        return

    clamped_angle = max(SERVO_MIN_ANGLE, min(SERVO_MAX_ANGLE, angle))
    if clamped_angle != angle:
        print(f"è§’åº¦{angle}ã‚’{clamped_angle}ã«è£œæ­£ã—ã¾ã—ãŸ")

    query = urlencode({"ch": channel, "angle": clamped_angle})
    url = f"{SERVO_API_BASE_URL}/servo?{query}"

    try:
        with urlopen(url, timeout=3) as response:
            response.read()
        print(f"ã‚µãƒ¼ãƒœé€ä¿¡æˆåŠŸ: axis={axis}, ch={channel}, angle={clamped_angle}")
    except URLError as exc:
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚µãƒ¼ãƒœé€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ ({url}): {exc}")


def handle_servo_commands(response_text: str) -> str:
    cleaned_lines: List[str] = []

    for raw_line in response_text.splitlines():
        for match in SERVO_COMMAND_PATTERN.finditer(raw_line):
            args = match.group(1)
            axis_match = SERVO_AXIS_PATTERN.search(args)
            angle_match = SERVO_ANGLE_PATTERN.search(args)

            if not axis_match or not angle_match:
                print(f"è­¦å‘Š: ã‚µãƒ¼ãƒœã‚³ãƒãƒ³ãƒ‰ã‚’è§£æã§ãã¾ã›ã‚“ã§ã—ãŸ: {match.group(0)}")
                continue

            axis = axis_match.group(1).lower()

            try:
                angle_str = angle_match.group(1).replace(' ', '')
                angle = int(angle_str)
            except ValueError:
                print(f"è­¦å‘Š: è§’åº¦ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {match.group(0)}")
                continue

            send_servo_command(axis, angle)

        cleaned_line = SERVO_COMMAND_PATTERN.sub('', raw_line).strip()
        if cleaned_line:
            cleaned_lines.append(cleaned_line)

    return "\n".join(cleaned_lines)


def take_photo_and_analyze(prompt: str = "") -> Tuple[str, str]:
    print("=" * 50)
    print("å†™çœŸæ’®å½±ã¨AIåˆ†æã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 50)
    
    photo_path = take_photo()
    result = analyze_photo_with_gemini(photo_path, prompt)
    
    print("\n" + "=" * 50)
    print("Gemini AIã®åˆ†æçµæœ:")
    print("=" * 50)
    print(result)
    print("=" * 50)
    speech_text = handle_servo_commands(result)
    synthesize_speech(speech_text)
    
    return photo_path, result


def record_voice_prompt(duration: float = RECORDING_DURATION, existing_stream: Optional[Any] = None, use_vad: bool = True) -> Optional[str]:
    if duration <= 0:
        raise ValueError(f"durationã¯æ­£ã®æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {duration}")
    
    os.makedirs(PROMPT_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(PROMPT_DIR, f"prompt_{timestamp}.wav")
    
    if use_vad:
        print(f"\néŸ³å£°ã‚’éŒ²éŸ³ã—ã¾ã™ï¼ˆæœ€å¤§{duration}ç§’ã€ç„¡éŸ³ã§è‡ªå‹•åœæ­¢ï¼‰...")
        print("è©±ã—å§‹ã‚ã¦ãã ã•ã„...")
        
        audio_data = []
        silent_samples = 0
        silence_samples_needed = int(VAD_SILENCE_DURATION * RECORDING_SAMPLE_RATE)
        min_samples = int(VAD_MIN_DURATION * RECORDING_SAMPLE_RATE)
        max_samples = int(duration * RECORDING_SAMPLE_RATE)
        
        if existing_stream is None:
            print("ã‚¨ãƒ©ãƒ¼: éŸ³å£°æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã§ã¯æ—¢å­˜ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒå¿…è¦ã§ã™")
            return None
        
        started = False
        
        while len(audio_data) < max_samples:
            chunk, _ = existing_stream.read(VAD_CHUNK_SIZE)
            chunk_audio = chunk[:, 0]
            audio_data.extend(chunk_audio)
            
            rms = np.sqrt(np.mean(chunk_audio ** 2))
            
            if not started and rms > VAD_SILENCE_THRESHOLD:
                started = True
                print("ğŸ¤ éŒ²éŸ³ä¸­...", end='', flush=True)
            
            if started:
                if rms < VAD_SILENCE_THRESHOLD:
                    silent_samples += len(chunk_audio)
                    progress = int((silent_samples / silence_samples_needed) * 10)
                    print(f"\rğŸ¤ éŒ²éŸ³ä¸­... {'.' * progress}{' ' * (10 - progress)}", end='', flush=True)
                else:
                    silent_samples = 0
                    print(f"\rğŸ¤ éŒ²éŸ³ä¸­...          ", end='', flush=True)
                
                if silent_samples >= silence_samples_needed and len(audio_data) >= min_samples:
                    print(f"\râœ“ ç„¡éŸ³ã‚’æ¤œå‡ºã€éŒ²éŸ³çµ‚äº†ï¼ˆ{len(audio_data) / RECORDING_SAMPLE_RATE:.1f}ç§’ï¼‰")
                    break
        
        audio = np.array(audio_data)
    else:
        print(f"\n{duration}ç§’é–“ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’éŒ²éŸ³ã—ã¾ã™...")
        print("è©±ã—å§‹ã‚ã¦ãã ã•ã„...")
        
        if existing_stream is not None:
            samples_needed = int(duration * RECORDING_SAMPLE_RATE)
            audio_data = []
            
            while len(audio_data) < samples_needed:
                chunk, _ = existing_stream.read(min(12000, samples_needed - len(audio_data)))
                audio_data.extend(chunk[:, 0])
            
            audio = np.array(audio_data[:samples_needed])
        else:
            audio = sd.rec(int(duration * RECORDING_SAMPLE_RATE), 
                           samplerate=RECORDING_SAMPLE_RATE, 
                           channels=1, 
                           dtype='float32')
            sd.wait()
            audio = audio[:, 0]
    
    audio_16k = librosa.resample(audio, 
                                  orig_sr=RECORDING_SAMPLE_RATE, 
                                  target_sr=TARGET_SAMPLE_RATE)
    
    sf.write(filename, audio_16k, TARGET_SAMPLE_RATE)
    
    if not use_vad:
        print(f"éŒ²éŸ³å®Œäº†: {filename}")
    return filename


def speech_to_text_with_gemini(audio_path: str) -> str:
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
    
    try:
        print("éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ä¸­...")
        
        audio_file = genai.upload_file(path=audio_path)
        response = model.generate_content([
            "ã“ã®éŸ³å£°ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€æ„å‘³ãŒèª­ã¿å–ã‚Œãªã„éƒ¨åˆ†ãŒã‚ã‚Œã°æ„è¨³ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚",
            audio_file
        ])
        
        text = response.text.strip()
        print(f"èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        
        audio_file.delete()
        
        return text
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: éŸ³å£°ã®ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise RuntimeError(f"éŸ³å£°ã®ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}") from e


def take_photo_and_analyze_with_voice() -> None:
    import lwake.features as features
    
    print("=" * 50)
    print("éŸ³å£°èªè­˜ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ã‚’è¨€ã£ã¦ãã ã•ã„...")
    print("=" * 50)
    
    print("ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    reference_features = []
    for i in range(1, 5):
        ref_path = f"{VOICE_EXAMPLES_DIR}/Sample{i}.wav"
        if os.path.exists(ref_path):
            feat = features.extract_embedding_features(path=ref_path)
            reference_features.append((f"Sample{i}.wav", feat))
            print(f"  {ref_path} èª­ã¿è¾¼ã¿å®Œäº†")
    
    if not reference_features:
        print("ã‚¨ãƒ©ãƒ¼: ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹éŸ³å£°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\n{len(reference_features)}å€‹ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    print(f"ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {REFERENCE_AUDIO_LENGTH + 0.5}ç§’")
    print(f"é–¾å€¤: {WAKE_THRESHOLD}")
    print("\nã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºã‚’é–‹å§‹ã—ã¾ã™...")
    
    buffer_duration = REFERENCE_AUDIO_LENGTH + 0.5
    slide_duration = 0.25
    
    buffer_samples = int(buffer_duration * RECORDING_SAMPLE_RATE)
    slide_samples = int(slide_duration * RECORDING_SAMPLE_RATE)
    
    audio_buffer = np.zeros(buffer_samples, dtype=np.float32)
    
    try:
        with sd.InputStream(samplerate=RECORDING_SAMPLE_RATE, channels=1, dtype=np.float32) as stream:
            while True:
                data, overflowed = stream.read(slide_samples)
                
                chunk = data[:, 0]
                
                audio_buffer = np.roll(audio_buffer, -len(chunk))
                audio_buffer[-len(chunk):] = chunk
                
                audio_16k = librosa.resample(audio_buffer, 
                                             orig_sr=RECORDING_SAMPLE_RATE, 
                                             target_sr=TARGET_SAMPLE_RATE)
                
                try:
                    feat = features.extract_embedding_features(y=audio_16k, sample_rate=TARGET_SAMPLE_RATE)
                except Exception as e:
                    continue
                
                detected = False
                min_distance = float('inf')
                best_match = None
                
                for ref_name, ref_feat in reference_features:
                    distance = features.dtw_cosine_normalized_distance(feat, ref_feat)
                    
                    if distance < min_distance:
                        min_distance = distance
                        best_match = ref_name
                    
                    if distance < WAKE_THRESHOLD:
                        print(f"\nâœ“ ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º! ({ref_name}, è·é›¢: {distance:.4f})")
                        detected = True
                        break
                
                if not detected and np.random.random() < 0.1:
                    print(f"  [ãƒ‡ãƒãƒƒã‚°] æœ€å°è·é›¢: {min_distance:.4f} ({best_match})", end='\r')
                
                if detected:
                    prompt_audio = record_voice_prompt(existing_stream=stream)
                    prompt_text = speech_to_text_with_gemini(prompt_audio)
                    
                    print("\nå†™çœŸã‚’æ’®å½±ã—ã¾ã™...")
                    photo_path = take_photo()
                    
                    result = analyze_photo_with_gemini(photo_path, prompt_text)
                    
                    print("\n" + "=" * 50)
                    print("Gemini AIã®åˆ†æçµæœ:")
                    print("=" * 50)
                    print(result)
                    print("=" * 50)
                    speech_text = handle_servo_commands(result)
                    synthesize_speech(speech_text)
                    
                    print(f"éŸ³å£°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {prompt_audio}")
                    
                    audio_buffer = np.zeros(buffer_samples, dtype=np.float32)
                    
                    print(f"\nå†åº¦ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ã‚’è¨€ã£ã¦ãã ã•ã„...")
    
    except KeyboardInterrupt:
        print("\n\néŸ³å£°èªè­˜ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    take_photo_and_analyze_with_voice()