import os
import subprocess
import time
import threading
import numpy as np

from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_cors import CORS
import google.generativeai as genai
from PIL import Image

# --- éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
import pyaudio
from faster_whisper import WhisperModel
import wave

# --- ã‚«ãƒ¡ãƒ©ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (picamera2ã‚’æ¨å¥¨) ---
try:
    from picamera2 import Picamera2
except ImportError:
    print("Warning: picamera2 not found. Install it for camera functionality.")

# --- è¨­å®šå€¤ ---
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
if not GOOGLE_API_KEY:
    print("ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

# APIè¨­å®š
genai.configure(api_key=GOOGLE_API_KEY)
MODEL_NAME = "gemini-flash-lite-latest"
gemini_model = genai.GenerativeModel(MODEL_NAME)

# Whisperè¨­å®š
# tiny, base, small, medium, largeã‹ã‚‰é¸æŠï¼ˆbaseã¯æ—¥æœ¬èªèªè­˜ãŒã‚ˆã‚Šè‰¯ã„ï¼‰
# Whisperè¨­å®š
# tiny, base, small, medium, largeã‹ã‚‰é¸æŠ
WHISPER_MODEL = WhisperModel("tiny", device="cpu", compute_type="int8")
SAMPLE_RATE = 44100  # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¬ãƒ¼ãƒˆ
CHUNK_SIZE = 1280    # mimi.pyã¨åŒã˜ã‚µã‚¤ã‚º
MICROPHONE_INDEX = 1  # ä½¿ç”¨ã™ã‚‹ãƒã‚¤ã‚¯ãƒ‡ãƒã‚¤ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„

# AquesTalkPiè¨­å®š
AQUESTALK_PATH = "/home/pi/pivot/aques/AquesTalkPi"

# ã‚«ãƒ¡ãƒ©è¨­å®š
IMAGE_PATH = r"/home/pi/pivot/temp_capture.jpg" # æ’®å½±ã—ãŸç”»åƒã‚’ä¿å­˜ã™ã‚‹ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
DEFAULT_PROMPT = "ã“ã®ç”»åƒã«ã¤ã„ã¦ä½•ã‹å°‹ã­ã¦ã„ã¾ã™ã‹ï¼Ÿ"

# --- Gemini ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆè¨˜æ†¶ä¿æŒç”¨ï¼‰ ---
# ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã¯ã€æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å«ã‚ã‚‹ã‹ã€Geminiãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ï¼ˆã“ã“ã§ã¯æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å«ã‚ã¾ã™ï¼‰
CHAT = gemini_model.start_chat(history=[])
print(f"âœ… Geminiãƒ¢ãƒ‡ãƒ«: {MODEL_NAME} ã§ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")

# --- Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
app = Flask(__name__)
CORS(app)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆéŸ³å£°èªè­˜åˆ¶å¾¡ç”¨ï¼‰
recognition_active = False
audio_stream = None
recognition_lock = threading.Lock()

# --- AquesTalkPi éŸ³å£°åˆæˆé–¢æ•° ---
def speak(text):
    """AquesTalkPiã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã—ã€aplayã§å‡ºåŠ›ã™ã‚‹."""
    # ã‚³ãƒãƒ³ãƒ‰ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é˜²ããŸã‚ã€subprocess.runã‚’ä½¿ç”¨
    try:
        # AquesTalkPiã‚³ãƒãƒ³ãƒ‰ã¨aplayã‚³ãƒãƒ³ãƒ‰ã‚’ãƒ‘ã‚¤ãƒ—ã§æ¥ç¶š
        process = subprocess.Popen(
            [AQUESTALK_PATH, text],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        subprocess.run(
            ['aplay'],
            stdin=process.stdout,
            check=True
        )
        process.stdout.close() # ãƒ‘ã‚¤ãƒ—ã‚’é–‰ã˜ã‚‹
        process.wait()
    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: AquesTalkPiã¾ãŸã¯aplayãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {AQUESTALK_PATH}")
    except Exception as e:
        print(f"éŸ³å£°å‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†é–¢æ•° ---
def enhance_audio(audio_data, sample_rate=44100, gain=2.0):
    """ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ å¢—å¹…"""
    try:
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ å¢—å¹…
        amplified = audio_data * gain
        amplified = np.clip(amplified, -32768, 32767).astype(np.int16)
        return amplified
    except Exception as e:
        print(f"ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return audio_data

# --- ã‚«ãƒ¡ãƒ©æ’®å½±é–¢æ•° ---
def take_picture(path=IMAGE_PATH):
    """Raspberry Pi Camera V1 (Picamera2) ã§ç”»åƒã‚’æ’®å½±ã™ã‚‹."""
    try:
        # Picamera2ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
        # V1ã‚«ãƒ¡ãƒ©ã®è§£åƒåº¦ã«åˆã‚ã›ã¦è¨­å®šã‚’èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“
        picam2 = Picamera2()
        config = picam2.create_still_configuration(main={"size": (640, 480)}) # V1ã‚«ãƒ¡ãƒ©ã¯æœ€å¤§5MP
        picam2.configure(config)
        picam2.start()
        time.sleep(2) # ã‚ªãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã¨éœ²å‡ºèª¿æ•´ã‚’å¾…ã¤
        
        picam2.capture_file(path)
        picam2.stop()
        print(f"ğŸ“¸ ç”»åƒã‚’æ’®å½±ã—ã€{path}ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return True
    except NameError:
        print("ã‚¨ãƒ©ãƒ¼: picamera2ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ¡ãƒ©æ©Ÿèƒ½ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        return False
    except Exception as e:
        print(f"ã‚«ãƒ¡ãƒ©æ’®å½±ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•° ---
def process_request(user_text, image_path):
    """Gemini APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã€å¿œç­”ã‚’éŸ³å£°ã§å‡ºåŠ›ã™ã‚‹."""
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
    try:
        image = Image.open(image_path)
        print("ç”»åƒã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    except Exception:
        print("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§é€ä¿¡ã—ã¾ã™ã€‚")
        image = None

    # RAG.txtã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
    rag_content = ""
    try:
        with open("/home/pi/pivot/RAG.txt", 'r', encoding='utf-8') as f:
            rag_content = f.read()
        print("ğŸ“š RAG.txtã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except Exception as e:
        print(f"âš ï¸ RAG.txtèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        rag_content = ""
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆRAG.txt + ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰
    if not user_text.strip():
        user_input = DEFAULT_PROMPT
    else:
        user_input = user_text
    
    # RAG.txtã®å†…å®¹ã‚’å‰ç½®ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    if rag_content:
        full_prompt = f"{rag_content}\n\n//What users say//\n{user_input}"
    else:
        full_prompt = user_input

    print(f"\nğŸ§  Geminiã«é€ä¿¡: {user_input} (RAGä»˜ã: {bool(rag_content)})")

    # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆ
    contents = [full_prompt]
    if image:
        contents.append(image)

    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œï¼ˆè¨˜æ†¶ä¿æŒã®ãŸã‚ chat.send_message ã‚’ä½¿ç”¨ï¼‰
    try:
        response = CHAT.send_message(contents)
        ai_response = response.text
        print(f"ğŸ¤– AIå¿œç­”: {ai_response}")
        speak(ai_response) # éŸ³å£°ã§èª­ã¿ä¸Šã’
        return {"status": "success", "ai_response": ai_response}
    except Exception as e:
        error_message = f"Gemini APIã‚¨ãƒ©ãƒ¼: {e}"
        print(error_message)
        speak("ã”ã‚ã‚“ãªã•ã„ã€‚AIã¨ã®é€šä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return {"status": "error", "ai_response": "AIã¨ã®é€šä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}


# --- HTTP ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.route('/clicked', methods=['POST'])
def handle_click():
    """HTTPã§POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã¨éŸ³å£°èªè­˜ã‚’é–‹å§‹"""
    global recognition_active, recognition_lock

    # åŒæ™‚å®Ÿè¡Œã‚’é˜²æ­¢: èªè­˜ä¸­ã¯è¿½åŠ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç„¡è¦–ã™ã‚‹
    with recognition_lock:
        if recognition_active:
            print("âš ï¸ æ—¢ã«éŸ³å£°èªè­˜ä¸­ã®ãŸã‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç„¡è¦–ã—ã¾ã™ã€‚")
            return jsonify({
                "status": "ignored",
                "message": "ç¾åœ¨éŸ³å£°èªè­˜ä¸­ã§ã™ã€‚å¾Œã§è©¦ã—ã¦ãã ã•ã„ã€‚",
                "transcript": "",
                "ai_response": ""
            }), 200
        # èªè­˜ã‚’é–‹å§‹ã™ã‚‹ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
        recognition_active = True

    try:
        print("ğŸŒ HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡ã—ã¾ã—ãŸï¼éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™...")
        result = start_voice_recognition()

        return jsonify({
            "status": "success",
            "message": "éŸ³å£°èªè­˜å®Œäº†",
            "transcript": result.get("transcript", ""),
            "ai_response": result.get("ai_response", "")
        })

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

    finally:
        # èªè­˜ãƒ•ãƒ©ã‚°ã‚’å¿…ãšã‚¯ãƒªã‚¢ã™ã‚‹
        with recognition_lock:
            recognition_active = False

def start_voice_recognition():
    """Whisperã‚’ä½¿ã£ã¦éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œã—ã¦AIå¿œç­”ã¾ã§å‡¦ç†"""
    global audio_stream
    
    if not audio_stream:
        raise Exception("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    print(f"ğŸ™ï¸  ä½•ã‹ãŠè©±ã—ãã ã•ã„... (7ç§’é–“éŒ²éŸ³, {SAMPLE_RATE}Hz)")
    
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    frames = []
    recording_duration = 7  # 7ç§’é–“éŒ²éŸ³ï¼ˆé•·ã‚ã«è¨­å®šï¼‰
    total_frames = int(SAMPLE_RATE / CHUNK_SIZE * recording_duration)
    
    try:
        for i in range(total_frames):
            data = audio_stream.read(CHUNK_SIZE, exception_on_overflow=False)
            frames.append(data)
    except Exception as e:
        print(f"éŸ³å£°éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "transcript": "",
            "ai_response": "éŸ³å£°éŒ²éŸ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        }
    
    print("ğŸ”„ Whisperã§éŸ³å£°èªè­˜ä¸­...")
    
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
    audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
    
    # éŸ³å£°ãƒ¬ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯
    audio_max = np.max(np.abs(audio_data))
    audio_mean = np.mean(np.abs(audio_data))
    print(f"ğŸ“Š éŸ³å£°ãƒ¬ãƒ™ãƒ« - æœ€å¤§: {audio_max}, å¹³å‡: {audio_mean:.1f}")
    
    if audio_max < 100:
        print("âš ï¸ éŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒéå¸¸ã«ä½ã„ã§ã™ã€‚ãƒã‚¤ã‚¯ã®éŸ³é‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    # éŸ³å£°å¼·åŒ–ï¼ˆã‚²ã‚¤ãƒ³ã‚’ä¸Šã’ã‚‹ï¼‰
    enhanced_audio = enhance_audio(audio_data, SAMPLE_RATE, gain=5.0)
    
    # float32ã«æ­£è¦åŒ–ï¼ˆWhisperã®å…¥åŠ›å½¢å¼ï¼‰
    audio_float = enhanced_audio.astype(np.float32) / 32768.0
    
    try:
        # Whisperã§éŸ³å£°èªè­˜ï¼ˆæ—¥æœ¬èªç‰¹åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        segments, info = WHISPER_MODEL.transcribe(
            audio_float, 
            beam_size=5, 
            language="ja",
            condition_on_previous_text=False,  # å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã«ä¾å­˜ã—ãªã„
            temperature=0.0,  # ç¢ºå®šçš„ãªçµæœ
            compression_ratio_threshold=2.4,  # æ—¥æœ¬èªç”¨ã«èª¿æ•´
            log_prob_threshold=-1.0,  # ãƒ­ã‚°ç¢ºç‡é–¾å€¤ã‚’ä¸‹ã’ã‚‹
            no_speech_threshold=0.6  # ç„¡éŸ³æ¤œå‡ºé–¾å€¤ã‚’ä¸Šã’ã‚‹
        )
        
        print(f"ğŸŒ æ¤œå‡ºè¨€èª: {info.language} (ç¢ºç‡: {info.language_probability:.2f})")
        
        # èªè­˜çµæœã‚’å–å¾—
        user_transcript = ""
        for segment in segments:
            print(f"ğŸ“ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: '{segment.text}' (ç¢ºç‡: {segment.avg_logprob:.2f})")
            user_transcript += segment.text + " "
        
        user_transcript = user_transcript.strip()
        
        if user_transcript:
            print(f"ğŸ“ èªè­˜çµæœ: {user_transcript}")
            
            # ç”»åƒæ’®å½±
            take_picture()
            
            # AIå¿œç­”ã‚’ç”Ÿæˆï¼ˆç”»åƒä»˜ãã§ï¼‰
            ai_response = ""
            try:
                response_data = process_request(user_transcript, IMAGE_PATH)
                ai_response = response_data.get("ai_response", "AIå¿œç­”ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                print(f"ğŸ¤– AIå¿œç­”: {ai_response}")
                speak(ai_response)
            except Exception as e:
                ai_response = f"AIã‚¨ãƒ©ãƒ¼: {e}"
                speak("ã”ã‚ã‚“ãªã•ã„ã€‚AIã¨ã®é€šä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                print(f"AIã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
            
            return {
                "transcript": user_transcript,
                "ai_response": ai_response
            }
        else:
            print("âš ï¸ ç™ºè¨€ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: éŸ³å£°æœ€å¤§ãƒ¬ãƒ™ãƒ«={audio_max}, å¹³å‡ãƒ¬ãƒ™ãƒ«={audio_mean:.1f}")
            print("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ãƒã‚¤ã‚¯ã«è¿‘ã¥ã„ã¦ã€ã¯ã£ãã‚Šã¨è©±ã—ã¦ãã ã•ã„")
            speak("èãå–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†å°‘ã—å¤§ããªå£°ã§ãŠè©±ã—ãã ã•ã„ã€‚")
            return {
                "transcript": "",
                "ai_response": "èãå–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†å°‘ã—å¤§ããªå£°ã§ãŠè©±ã—ãã ã•ã„ã€‚"
            }
            
    except Exception as e:
        print(f"WhisperéŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
        speak("éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return {
            "transcript": "",
            "ai_response": "éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        }

# --- ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— (HTTP ã‚µãƒ¼ãƒãƒ¼) ---
def main_loop():
    global audio_stream
    
    # PyAudioè¨­å®š
    p = pyaudio.PyAudio()
    audio_stream = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=SAMPLE_RATE,
        input=True,
        frames_per_buffer=CHUNK_SIZE,
        input_device_index=MICROPHONE_INDEX,
    )
    
    print(f"\nğŸ¤– WhisperéŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    print(f"ğŸ¤ HTTPã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­...")
    print(f"ğŸŒ http://pi.local:8100/clicked ã«POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã¦ãã ã•ã„")
    
    # HTTPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    try:
        app.run(host='0.0.0.0', port=8100, debug=False)
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        audio_stream.stop_stream()
        audio_stream.close()
        p.terminate()



if __name__ == "__main__":
    main_loop()